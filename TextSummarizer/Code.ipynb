{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeBM25Score(R,S,idf,tf,k,b,avgDL):\n",
    "    S = S.split()\n",
    "    R = R.split()\n",
    "    score = 0\n",
    "    for ws in S:\n",
    "        numerator = tf[ws] * (k + 1)\n",
    "        denominator = tf[ws] + k * (1- b + b * (len(R)/avgDL))\n",
    "        score += computeIdf(ws,idf)  * numerator/denominator\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeIdf(word,idf):\n",
    "    if word in idf:\n",
    "        #print(idf[word])\n",
    "        return idf[word]\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "    file = open(path,\"r\")\n",
    "    sentences = []\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.strip():\n",
    "            if line == \"@highlight\":\n",
    "                break\n",
    "            sentences.append(line)\n",
    "            \n",
    "    file.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopping(sentences):\n",
    "    stoppedSent = []\n",
    "    stopWords =[]\n",
    "    file = open(\"stop-word-list.txt\", 'r')\n",
    "    for line in file:\n",
    "        stopWords.append(line.strip())\n",
    "    for s in sentences:\n",
    "        temp = s.split()\n",
    "        sent = \"\"\n",
    "        for w in temp:\n",
    "            if w not in stopWords:\n",
    "                sent += w + \" \"\n",
    "        stoppedSent.append(sent)\n",
    "    return stoppedSent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def stemming(sentences):\n",
    "    stemmedSent = []\n",
    "    ps = PorterStemmer()\n",
    "    for s in sentences:\n",
    "        temp = s.split()\n",
    "        stemmedWord = \"\"\n",
    "        for w in temp:\n",
    "            stemmedWord += ps.stem(w) + \" \"\n",
    "        stemmedSent.append(stemmedWord)\n",
    "    return stemmedSent\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSimilarityMatrix(sentences,idf,k,b):\n",
    "    BM25_scores = [[0 for x in range(len(sentences))] for y in range(len(sentences))]\n",
    "    avgDL = computeAvgDL(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                BM25_scores[i][j] = 1\n",
    "            else:\n",
    "                \n",
    "                tf = generateTF(sentences[i],sentences[j])\n",
    "                BM25_scores[i][j] = computeBM25Score(sentences[i],sentences[j],idf,tf,k,b,avgDL)\n",
    "    #print(BM25_scores)\n",
    "    return BM25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeAvgDL(sentences):\n",
    "    sum =  0\n",
    "    for s in sentences:\n",
    "        sum += len(s.split())\n",
    "    avg = sum/len(sentences)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTF(R,S):\n",
    "    tf = {}\n",
    "    #print(R,S)\n",
    "    S = S.split()\n",
    "    R = R.split()\n",
    "    for ws in S:\n",
    "        if ws in R:\n",
    "            if ws in tf:\n",
    "                tf[ws] += 1\n",
    "            else:\n",
    "                tf[ws] = 1\n",
    "        else:\n",
    "            tf[ws] = 0\n",
    "    #print(tf)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def textRank(sentences,BM25_Score,d):\n",
    "    ws = {}\n",
    "    #print(BM25_Score)\n",
    "    #assigning initial pagerank \n",
    "    for i in range(len(sentences)):\n",
    "        ws[i] = 1/len(sentences)\n",
    "    currSum = 0\n",
    "    prevSum = -1\n",
    "    \n",
    "    count = 0\n",
    "    while((currSum - prevSum) == 0 or count != 10 ):\n",
    "        prevSum = currSum\n",
    "        count += 1\n",
    "        for i in range(len(sentences)):\n",
    "            inlinkContri = 0\n",
    "            for j in range(len(sentences)):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                outlinkContrib = 0\n",
    "                for k in range(len(sentences)):\n",
    "                    if k == j:\n",
    "                        continue\n",
    "                    outlinkContrib += BM25_Score[j][k]\n",
    "                if outlinkContrib != 0:\n",
    "                    inlinkContri += (BM25_Score[j][i]/outlinkContrib) * ws[j]\n",
    "                else: \n",
    "                    inlinkContri = 0\n",
    "            ws[i] =  (1-d) + d * inlinkContri\n",
    "            currSum += ws[i]\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "from os.path import join\n",
    "def generateIdfDict():\n",
    "    idfFile = 'resources/idfFile/finalIdfscore.csv'\n",
    "    file = join(getcwd(), idfFile)\n",
    "    idfDict = {}\n",
    "    f = open(file,'r',encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        lineArr = line.split(',')\n",
    "        idfDict[lineArr[0]] = float(lineArr[1])\n",
    "    f.close()\n",
    "    print(\"idf done\")\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def summarizeWithoutFilter(path, outPath, idf, ratio):\n",
    "    b = 0.75\n",
    "    k = 1.2\n",
    "    d = 0.85\n",
    "    sentences = readFile(path)\n",
    "    stoppedSentences = stopping(sentences)\n",
    "    stemmedSentences = stemming(sentences)\n",
    "    summary = []\n",
    "    #print(stemmedSentences)\n",
    "#     idf = generateIdfDict()\n",
    "    BM25_Score = createSimilarityMatrix(stemmedSentences,idf,k,b)\n",
    "    sentenceScore = textRank(stemmedSentences,BM25_Score,d)\n",
    "    file = open(outPath,\"w+\")\n",
    "    n = int(ratio * len(sentences))\n",
    "    \n",
    "    #print(sentenceScore)\n",
    "#     sentenceScore = dict(sorted(sentenceScore.items(), key=lambda x:x[1], reverse=True)[:n])\n",
    "#     sentenceScore = sorted(sentenceScore.items(), key=lambda x:x[0])\n",
    "    sentenceScore = sorted(sentenceScore.items(), key=lambda x:x[1], reverse=True)\n",
    "#     print(sentenceScore)\n",
    "    summaryIndexArray = calculateTopKSentencesWithoutFilter(sentences, sentenceScore, n)\n",
    "    sortedSummaryIndexArray = sorted(summaryIndexArray, key=lambda x:x[0], reverse=False)\n",
    "    for line in sortedSummaryIndexArray:\n",
    "        summary.append(sentences[line[0]])\n",
    "    for line in summary:\n",
    "#         i = index[0]\n",
    "        file.write(line + '\\n')\n",
    "#         file.write(sentences[i])\n",
    "#         print(sentences[i])\n",
    "#         file.write(\"\\n\")\n",
    "#         print(\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def summarize(path, outPath, idf, ratio):\n",
    "    b = 0.75\n",
    "    k = 1.2\n",
    "    d = 0.85\n",
    "    sentences = readFile(path)\n",
    "    stoppedSentences = stopping(sentences)\n",
    "    stemmedSentences = stemming(sentences)\n",
    "    summary = []\n",
    "    #print(stemmedSentences)\n",
    "#     idf = generateIdfDict()\n",
    "    BM25_Score = createSimilarityMatrix(stemmedSentences,idf,k,b)\n",
    "    sentenceScore = textRank(stemmedSentences,BM25_Score,d)\n",
    "    file = open(outPath,\"w+\")\n",
    "    n = int(ratio * len(sentences))\n",
    "    \n",
    "    #print(sentenceScore)\n",
    "#     sentenceScore = dict(sorted(sentenceScore.items(), key=lambda x:x[1], reverse=True)[:n])\n",
    "#     sentenceScore = sorted(sentenceScore.items(), key=lambda x:x[0])\n",
    "    sentenceScore = sorted(sentenceScore.items(), key=lambda x:x[1], reverse=True)\n",
    "#     print(sentenceScore)\n",
    "    summaryIndexArray = calculateTopKSentences(sentences, sentenceScore, n)\n",
    "    sortedSummaryIndexArray = sorted(summaryIndexArray, key=lambda x:x[0], reverse=False)\n",
    "    for line in sortedSummaryIndexArray:\n",
    "        summary.append(sentences[line[0]])\n",
    "    for line in summary:\n",
    "#         i = index[0]\n",
    "        file.write(line + '\\n')\n",
    "#         file.write(sentences[i])\n",
    "#         print(sentences[i])\n",
    "#         file.write(\"\\n\")\n",
    "#         print(\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize(\"C:\\\\NewsData\\\\NewsFile4.txt\",\"C:\\\\NewsData\\\\SummryNewsFile4.txt\",0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEX RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  read a single file and returns fileContent and summary content  \n",
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "def readFromFile(filename):\n",
    "#     docSummaryDict = {}\n",
    "#     docSentencesDict = {}\n",
    "    fileContent = []\n",
    "    summaryContent = []\n",
    "    summaryline = False\n",
    "    for line in open(filename,'r'): \n",
    "        if line != '\\n':\n",
    "            if '@highlight' in line:\n",
    "                summaryline = True\n",
    "                continue\n",
    "            if not summaryline:\n",
    "                fileContent.append(line.strip())\n",
    "            else:\n",
    "                summaryline = False\n",
    "                summaryContent.append(line)\n",
    "    return fileContent, summaryContent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to read stop words from a text file\n",
    "def readStopWords(filename):\n",
    "    text = []\n",
    "    if(filename.endswith('txt')):\n",
    "        file = open(filename, 'r')\n",
    "        for line in file:\n",
    "            text.append(line.strip())\n",
    "        return text\n",
    "    else:\n",
    "        return None;\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "def removeStopWordsFromFile(fileContentArray):\n",
    " #  list of stopWords ( removed following words from list -> eight, eleven, fifteen, first, five, forty, four, nine, \n",
    "#   one, six, sixty, twelve, twenty, two, ten, )\n",
    "# http://xpo6.com/download-stop-word-list/\n",
    "    stopWordsFile = 'stop-word-list.txt'\n",
    "    stopWords = readStopWords(stopWordsFile)\n",
    "    if stopWords is None:\n",
    "        raise Exception('Couldn\\'t parse the given file. Stop Words list is empty. Please provide a text file to parse.')\n",
    "    modifiedTextArr = []\n",
    "    for text in fileContentArray:\n",
    "        modifiedText = ''\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            if word not in stopWords:\n",
    "                modifiedText += word + ' '\n",
    "        modifiedTextArr.append(modifiedText)\n",
    "    return modifiedTextArr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "# creating stemmed text and stemmedTextDict(for term frequency)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "def getStemmedTextForFile(fileContentArray):\n",
    "    ps = PorterStemmer()\n",
    "    modifiedTextArr = []\n",
    "    for text in fileContentArray:\n",
    "        stemmedText = ''\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            stemmedWord = ps.stem(word)\n",
    "            stemmedText += stemmedWord + ' '\n",
    "        modifiedTextArr.append(stemmedText)\n",
    "    return modifiedTextArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from math import pow, log, sqrt\n",
    "from nltk import word_tokenize\n",
    "def idfModifiedCosine(sentence1,sentence2, idfDict):\n",
    "#     print('sentence1 is ' + sentence1)\n",
    "#     print('sentence2 is ' + sentence2)\n",
    "    sent1TFDict = Counter()\n",
    "    sent2TFDict = Counter()\n",
    "#     idfDict = {}\n",
    "    wordsInbothSentences = None\n",
    "    sentence1Arr = word_tokenize(sentence1)\n",
    "    sentence2Arr = word_tokenize(sentence2)\n",
    "    for word in sentence1Arr:\n",
    "        sent1TFDict[word] += 1\n",
    "        if wordsInbothSentences is None:\n",
    "            wordsInbothSentences = {word}\n",
    "        else:\n",
    "            wordsInbothSentences = wordsInbothSentences | {word}\n",
    "#         if word not in idfDict:\n",
    "# #             print('calculate idf for word : ' +  word + ' ' +  str(termDocfreqDict[word]))\n",
    "#             idfDict[word] = log(totalNumberOfDocs / termDocfreqDict[word])\n",
    "    for word in sentence2Arr:\n",
    "        sent2TFDict[word] += 1\n",
    "        if wordsInbothSentences is None:\n",
    "            wordsInbothSentences = {word}\n",
    "        else:\n",
    "            wordsInbothSentences = wordsInbothSentences | {word}\n",
    "#         if word not in idfDict:\n",
    "# #             print('calculate idf for word : ' +  word + ' ' +  str(termDocfreqDict[word]))\n",
    "#             idfDict[word] = log(totalNumberOfDocs / termDocfreqDict[word])\n",
    "    num  = 0\n",
    "    denSent1 = 0\n",
    "    denSent2 = 0\n",
    "    for word in wordsInbothSentences:\n",
    "        num += sent1TFDict.get(word,0) * sent2TFDict.get(word,0) * pow(idfDict.get(word,1),2)\n",
    "        denSent1 += pow(sent1TFDict.get(word,0) * idfDict.get(word,1),2)\n",
    "        denSent2 += pow(sent2TFDict.get(word,0) * idfDict.get(word,1),2)              \n",
    "    return num / (sqrt(denSent1) * sqrt(denSent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  implementing matrix Product\n",
    "import numpy as np\n",
    "def matrixProduct(matA, matB):\n",
    "    return np.matmul(matA, matB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matrixTranspose(mat):\n",
    "    return np.transpose(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matrixDifference(mat1,mat2):\n",
    "    return np.subtract(mat1,mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import pow\n",
    "# http://www.personal.soton.ac.uk/jav/soton/HELM/workbooks/workbook_30/30_4_matrx_norms.pdf\n",
    "def calculateEucledeanNorm(mat):\n",
    "    val = 0\n",
    "    for row in mat:\n",
    "        for col in range(len(row)):\n",
    "            val += pow(row[col],2)\n",
    "    return pow(val,1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  implementing power method\n",
    "def powerMethod(cosineMatrix, N, tolerance):\n",
    "    cosineMatrixTranspose = matrixTranspose(cosineMatrix)\n",
    "    initializeP = [[(1/N) for x in range(N)] for y in range(N)]\n",
    "    t = 0\n",
    "    currentP = None\n",
    "    while True:\n",
    "        if currentP is None:\n",
    "            oldP = initializeP\n",
    "        else:\n",
    "            oldP = currentP\n",
    "        t += 1\n",
    "        currentP = matrixProduct(cosineMatrixTranspose,oldP)\n",
    "        differenceMatrix = matrixDifference(currentP, oldP)\n",
    "        difference = calculateEucledeanNorm(differenceMatrix)      \n",
    "        if difference < tolerance:\n",
    "            return currentP  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Lex Rank\n",
    "#  need to check from where do we get tolerance, threshold\n",
    "#  idf-modified-cosine\n",
    "#  We might get divide by zero error, need to think how to avoid that.\n",
    "\n",
    "def lexRank(sentences,threshold, tolerance, idfDict):\n",
    "    n = len(sentences)\n",
    "    cosineMatrix = [[0 for x in range(n)] for y in range(n)]\n",
    "    degree = [1 for x in range(n)]\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            cosineMatrix[i][j] = idfModifiedCosine(sentences[i],sentences[j], idfDict)\n",
    "            if cosineMatrix[i][j] > threshold:\n",
    "                cosineMatrix[i][j] = 1\n",
    "                degree[i] += 1\n",
    "            else:\n",
    "                cosineMatrix[i][j] = 0\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            cosineMatrix[i][j] = cosineMatrix[i][j]/degree[i]\n",
    "    L = powerMethod(cosineMatrix, n, tolerance)\n",
    "    return L, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  storing summarisation to disk\n",
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "def writeResultsToFile(filename, summarySentences):\n",
    "    if(len(summarySentences)>0):\n",
    "        f = open(filename,'w')\n",
    "        for line in summarySentences:\n",
    "            f.write(line + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def computeLogEntropy(probDict, sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    entropySum = 0\n",
    "    for word in words:\n",
    "        entropySum += -1 * (probDict[word] * log(probDict[word]))\n",
    "    a = round(entropySum,1)\n",
    "#     print(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "def getSentencesEntropy(fileContentArray, scoreIndexArraySorted):\n",
    "    entropyIndexedSentenceArray = []\n",
    "    probDict = {}\n",
    "    count = 0\n",
    "    for sentence in fileContentArray:\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word in probDict:\n",
    "                probDict[word] += 1\n",
    "            else:\n",
    "                probDict[word] = 1\n",
    "            count += 1\n",
    "    for word in probDict:\n",
    "        probDict[word] = probDict[word]/count\n",
    "\n",
    "    for index, score in scoreIndexArraySorted:\n",
    "        sentence = fileContentArray[index]\n",
    "        entropyIndexedSentenceArray.append((index, score, computeLogEntropy(probDict, sentence)))\n",
    "    return entropyIndexedSentenceArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateTopKSentencesWithoutFilter(fileContentArray, scoreIndexArraySorted, summaryCount):\n",
    "#     scoreIndexArraySorted = getSentencesEntropy(fileContentArray, scoreIndexArraySorted)\n",
    "    index = 0\n",
    "    summaryIndexArray=[]\n",
    "#     entropyScore = []\n",
    "    for line in scoreIndexArraySorted:\n",
    "        if index < summaryCount:\n",
    "#             if format(line[2],'.2f') in entropyScore:\n",
    "#                 continue\n",
    "            summaryIndexArray.append(line)\n",
    "            index += 1\n",
    "#             entropyScore.append(format(line[2],'.2f'))\n",
    "        else:\n",
    "            break;\n",
    "    return summaryIndexArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateTopKSentences(fileContentArray, scoreIndexArraySorted, summaryCount):\n",
    "    scoreIndexArraySorted = getSentencesEntropy(fileContentArray, scoreIndexArraySorted)\n",
    "#     s = sorted(scoreIndexArraySorted, key=lambda x:x[2], reverse=True)\n",
    "#     print(s)\n",
    "    index = 0\n",
    "    summaryIndexArray=[]\n",
    "    entropyScore = []\n",
    "    for line in scoreIndexArraySorted:\n",
    "        if index < summaryCount:\n",
    "            if line[2] in entropyScore:\n",
    "                continue\n",
    "            summaryIndexArray.append(line)\n",
    "            index += 1\n",
    "            entropyScore.append(line[2])\n",
    "        else:\n",
    "            break;\n",
    "    return summaryIndexArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexSummariesWithoutFilter(inputFile, outputFile, outputHumanSummary, idfDict, cutoff = .20,threshold=0.1, tolerance=0.8):\n",
    "    fileContentArray, summaryContentArray = readFromFile(inputFile)\n",
    "    fileContentArrayUnStopped = removeStopWordsFromFile(fileContentArray)\n",
    "    fileContentArrayStemmed = getStemmedTextForFile(fileContentArrayUnStopped)\n",
    "    L, degree = lexRank(fileContentArrayStemmed,threshold, tolerance, idfDict)\n",
    "    scoreIndexArray = []\n",
    "    summarySentences = []\n",
    "    index=0\n",
    "    for row in L:\n",
    "        scoreIndexArray.append((index, row[0]))\n",
    "        index += 1\n",
    "        \n",
    "    scoreIndexArraySorted = sorted(scoreIndexArray, key=lambda x:x[1], reverse=True)\n",
    "    summaryCount = int (len(fileContentArray) * cutoff)\n",
    "\n",
    "    summaryIndexArray = calculateTopKSentencesWithoutFilter(fileContentArray, scoreIndexArraySorted, summaryCount)\n",
    "    sortedSummaryIndexArray = sorted(summaryIndexArray, key=lambda x:x[0], reverse=False)\n",
    "    for line in sortedSummaryIndexArray:\n",
    "        summarySentences.append(fileContentArray[line[0]])\n",
    "        \n",
    "    writeResultsToFile(outputFile, summarySentences)\n",
    "    writeResultsToFile(outputHumanSummary, summaryContentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexSummaries(inputFile, outputFile, outputHumanSummary, idfDict, cutoff = .20,threshold=0.1, tolerance=0.8):\n",
    "    fileContentArray, summaryContentArray = readFromFile(inputFile)\n",
    "    fileContentArrayUnStopped = removeStopWordsFromFile(fileContentArray)\n",
    "    fileContentArrayStemmed = getStemmedTextForFile(fileContentArrayUnStopped)\n",
    "    L, degree = lexRank(fileContentArrayStemmed,threshold, tolerance, idfDict)\n",
    "    scoreIndexArray = []\n",
    "    summarySentences = []\n",
    "    index=0\n",
    "    for row in L:\n",
    "        scoreIndexArray.append((index, row[0]))\n",
    "        index += 1\n",
    "        \n",
    "    scoreIndexArraySorted = sorted(scoreIndexArray, key=lambda x:x[1], reverse=True)\n",
    "    summaryCount = int (len(fileContentArray) * cutoff)\n",
    "\n",
    "    summaryIndexArray = calculateTopKSentences(fileContentArray, scoreIndexArraySorted, summaryCount)\n",
    "    sortedSummaryIndexArray = sorted(summaryIndexArray, key=lambda x:x[0], reverse=False)\n",
    "    for line in sortedSummaryIndexArray:\n",
    "        summarySentences.append(fileContentArray[line[0]])\n",
    "    \n",
    "    writeResultsToFile(outputFile, summarySentences)\n",
    "    writeResultsToFile(outputHumanSummary, summaryContentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lexSummaries(inputFile, outputFile, outputHumanSummary, cutoff = 20,threshold=0.1, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "def readResults(filename):\n",
    "    allSentences = []\n",
    "    f = open(filename,'r')\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            allSentences.append(line.strip())\n",
    "    f.close()\n",
    "    return allSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def getUnigrams(textArr):\n",
    "    text = ''.join(textArr)\n",
    "    unigrams = word_tokenize(text)\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  implementing ROGUE score\n",
    "from nltk import bigrams\n",
    "def getBigrams(textArr):\n",
    "    bigrm = list(bigrams(getUnigrams(textArr)))\n",
    "    return bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LCS(sentence1,sentence2):\n",
    "    m = len(sentence1)\n",
    "    n = len(sentence2)\n",
    "    b = [[0 for i in range(0,n+1)] for j in range(0,m+1)]\n",
    "    c = [[0 for i in range(0,n+1)] for j in range(0,m+1)]\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            if sentence1[i-1] == sentence2[j-1]:\n",
    "                c[i][j] = c[i-1][j-1]+1\n",
    "                b[i][j] = 1\n",
    "            elif c[i-1][j] >= c[i][j-1]:\n",
    "                c[i][j] = c[i-1][j]\n",
    "                b[i][j] = 2\n",
    "            else:\n",
    "                c[i][j] = c[i][j-1]\n",
    "                b[i][j] = 3\n",
    "    return c[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LCSScore(refSummary, candidateSummary):\n",
    "    lscscore = LCS(refSummary,candidateSummary)\n",
    "    RLCS = lscscore/len(refSummary)\n",
    "    return RLCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeRougeLScore(candidateSummary, refSummary):\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for systemLine in candidateSummary:\n",
    "        for humanLine in refSummary:\n",
    "            count += 1\n",
    "            score += LCSScore(humanLine, systemLine)   \n",
    "    RougeL = score/count\n",
    "    return RougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeRouge2Score(candidateSummaryBigrams, refSummaryBigrams):\n",
    "    overlappingLexBigrams = []\n",
    "    for bigram in candidateSummaryBigrams:\n",
    "        if bigram in refSummaryBigrams:\n",
    "            overlappingLexBigrams.append(bigram)\n",
    "    Rouge2Lex = len(overlappingLexBigrams)/len(refSummaryBigrams)\n",
    "    return Rouge2Lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeRougeScore(lexRankOutputFile, textRankOutputFile, humanGeneratedFile):\n",
    "    systemGeneratedLex = readResults(lexRankOutputFile)\n",
    "    systemGeneratedText = readResults(textRankOutputFile)\n",
    "    humanGenerated = readResults(humanGeneratedFile)\n",
    "\n",
    "    # for ROUGE-2\n",
    "    systemGeneratedLexBigrams = getBigrams(systemGeneratedLex)\n",
    "    systemGeneratedTextBigrams = getBigrams(systemGeneratedText)\n",
    "    humanGeneratedBigrams = getBigrams(humanGenerated)\n",
    "\n",
    "    Rouge2Lex = computeRouge2Score(systemGeneratedLexBigrams, humanGeneratedBigrams)\n",
    "    Rouge2Text = computeRouge2Score(systemGeneratedTextBigrams, humanGeneratedBigrams)\n",
    "    \n",
    "#     for ROUGE-L Score\n",
    "    RougeLLex = computeRougeLScore(systemGeneratedLex, humanGenerated)\n",
    "    RougeLText = computeRougeLScore(systemGeneratedText, humanGenerated)\n",
    "\n",
    "#     summaryQualityLex = (RougeLLex+Rouge2Lex)/2\n",
    "#     summaryQualityText = (RougeLText+Rouge2Text)/2\n",
    "\n",
    "    return Rouge2Lex, Rouge2Text, RougeLLex, RougeLText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLexSummaryQuality(lexRankOutputFile, humanGeneratedFile):\n",
    "    systemGeneratedLex = readResults(lexRankOutputFile)\n",
    "    humanGenerated = readResults(humanGeneratedFile)\n",
    "\n",
    "    # for ROUGE-2\n",
    "    systemGeneratedLexBigrams = getBigrams(systemGeneratedLex)\n",
    "    humanGeneratedBigrams = getBigrams(humanGenerated)\n",
    "\n",
    "    Rouge2Lex = computeRouge2Score(systemGeneratedLexBigrams, humanGeneratedBigrams)\n",
    "    \n",
    "#     for ROUGE-L Score\n",
    "    RougeLLex = computeRougeLScore(systemGeneratedLex, humanGenerated)\n",
    "\n",
    "    summaryQualityLex = (RougeLLex+Rouge2Lex)/2\n",
    "\n",
    "    return summaryQualityLex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Summarizer with 2 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf done\n"
     ]
    }
   ],
   "source": [
    "idfDict = generateIdfDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "\n",
    "inputPath = 'resources/InputFiles/'\n",
    "outputLexPath = 'resources/OutputFiles/LexRankSummary/'\n",
    "outputTextPath = 'resources/OutputFiles/TextRankSummary/'\n",
    "outputHumanPath = 'resources/HumanSummaries/'\n",
    "# outputQualityFileName = 'resources/OutputFiles/qualityScores.txt'\n",
    "rouge2Filename = 'resources/OutputFiles/rouge2scores.txt'\n",
    "rougeLFilename = 'resources/OutputFiles/rougeLscores.txt'\n",
    "\n",
    "inputFilePath = join(getcwd(), inputPath)\n",
    "outputRouge2FilePath = join(getcwd(), rouge2Filename)\n",
    "outputRougeLFilePath = join(getcwd(), rougeLFilename)\n",
    "cutOff = .20\n",
    "\n",
    "allFiles = [f for f in listdir(inputFilePath) if not (f.startswith('.')) and isfile(join(inputFilePath, f))]\n",
    "# qualityScore = []\n",
    "rouge2score = []\n",
    "rougeLscore = []\n",
    "index = 0\n",
    "for file in allFiles:\n",
    "    try: \n",
    "        inputFileName = join(inputFilePath, file)\n",
    "        outputLexFileName = join(getcwd(), outputLexPath, file)\n",
    "        outputTextFileName = join(getcwd(), outputTextPath, file)\n",
    "        outputHumanFileName = join(getcwd(), outputHumanPath, file)\n",
    "    #     Lex Rank\n",
    "        lexSummaries(inputFileName, outputLexFileName, outputHumanFileName, idfDict, cutOff)\n",
    "    #     Text Rank\n",
    "        summarize(inputFileName,outputTextFileName,idfDict, cutOff)\n",
    "    #     quality score\n",
    "        Rouge2Lex, Rouge2Text, RougeLLex, RougeLText = computeRougeScore(outputLexFileName, outputTextFileName, outputHumanFileName)\n",
    "        rouge2score.append((file, Rouge2Lex,Rouge2Text))\n",
    "        rougeLscore.append((file, RougeLLex,RougeLText))\n",
    "        index += 1\n",
    "    except:\n",
    "        print('file number ' + str(index) + ' name '+ str(file) + ' got an exception')\n",
    "        continue\n",
    "        \n",
    "rouge2File = open(outputRouge2FilePath,'w')\n",
    "for scores in rouge2score:\n",
    "    rouge2File.write(scores[0] + ' , ' + str(scores[1]) + ' , ' + str(scores[2]) + '\\n')\n",
    "rouge2File.close()\n",
    "\n",
    "rougeLFile = open(outputRougeLFilePath, 'w')\n",
    "for scores in rougeLscore:\n",
    "    rougeLFile.write(scores[0] + ' , ' + str(scores[1]) + ' , ' + str(scores[2]) + '\\n')\n",
    "rougeLFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to convert Text Summary to Audio Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "def textToSpeech(text, outputFile):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readSummary(inputFileName):\n",
    "    text = ''\n",
    "    f = open(inputFileName,'r')\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            text += line.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "inputLexPath = 'resources/OutputFiles/LexRankSummary/'\n",
    "inputTextPath = 'resources/OutputFiles/TextRankSummary/'\n",
    "outputLexPath = 'resources/OutputFiles/LexRankAudio/'\n",
    "outputTextPath = 'resources/OutputFiles/TextRankAudio/'\n",
    "inputLexFilePath = join(getcwd(), inputLexPath)\n",
    "inputTextFilePath = join(getcwd(), inputTextPath)\n",
    "allLexFiles = [f for f in listdir(inputLexFilePath) if not (f.startswith('.')) and isfile(join(inputLexFilePath, f))]\n",
    "allTextFiles = [f for f in listdir(inputTextFilePath) if not (f.startswith('.')) and isfile(join(inputTextFilePath, f))]\n",
    "for file in allLexFiles:\n",
    "    try: \n",
    "        inputFileName = join(inputLexFilePath, file)\n",
    "        text = readSummary(inputFileName)\n",
    "        textToSpeech(text,outputLexPath+file.split('.txt')[0]+'.mp3')\n",
    "    except:\n",
    "        print('file ' + file + ' got an exception')\n",
    "        continue\n",
    "for file in allTextFiles:\n",
    "    try: \n",
    "        inputFileName = join(inputTextFilePath, file)\n",
    "        text = readSummary(inputFileName)\n",
    "        textToSpeech(text,outputTextPath+file.split('.txt')[0]+'.mp3')\n",
    "    except:\n",
    "        print('file ' + file + ' got an exception')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Code to convert Text Summary to Braille Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateBrailleDict():\n",
    "    # ASCII\n",
    "    asciicodes = [' ','!','\"','#','$','%','&','(',')','*','+',',','-','.','/','0','1','2','3','4','5','6',\n",
    "                  '7','8','9',':',';','<','=','>','?','@','a','b','c','d','e','f','g','h','i','j','k','l',\n",
    "                  'm','n','o','p','q','r','s','t','u','v','w','x','y','z','[','\\\\',']','^','_']\n",
    "    # Braille symbols\n",
    "    brailles = [' ','⠮','⠐','⠼','⠫','⠩','⠯','⠷','⠾','⠡','⠬','⠠','⠤','⠨','⠌','⠴','⠂','⠆','⠒','⠲','⠢','⠖',\n",
    "                '⠶','⠦','⠔','⠱','⠰','⠣','⠿','⠜','⠹','⠈','⠁','⠃','⠉','⠙','⠑','⠋','⠛','⠓','⠊','⠚','⠅','⠇',\n",
    "                '⠍','⠝','⠕','⠏','⠟','⠗','⠎','⠞','⠥','⠧','⠺','⠭','⠽','⠵','⠪','⠳','⠻','⠘','⠸']\n",
    "    translateDict = {}\n",
    "    for index,chr  in enumerate(asciicodes):\n",
    "    #     print(chr,len(chr))\n",
    "        translateDict[chr] =  brailles[index]\n",
    "        transtab = str.maketrans(translateDict)\n",
    "    return transtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textToBraille(fileName, outputPath):\n",
    "    transtab=generateBrailleDict()\n",
    "    file = open(fileName,'r')\n",
    "    file2 = open(outputPath + fileName.split('/')[-1],'w', encoding='utf-16')\n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        file2.write(line.translate(transtab))\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "inputLexPath = 'resources/OutputFiles/LexRankSummary/'\n",
    "inputTextPath = 'resources/OutputFiles/TextRankSummary/'\n",
    "outputLexPath = 'resources/OutputFiles/LexRankBraille/'\n",
    "outputTextPath = 'resources/OutputFiles/TextRankBraille/'\n",
    "inputLexFilePath = join(getcwd(), inputLexPath)\n",
    "inputTextFilePath = join(getcwd(), inputTextPath)\n",
    "allLexFiles = [f for f in listdir(inputLexFilePath) if not (f.startswith('.')) and isfile(join(inputLexFilePath, f))]\n",
    "allTextFiles = [f for f in listdir(inputTextFilePath) if not (f.startswith('.')) and isfile(join(inputTextFilePath, f))]\n",
    "for file in allLexFiles:\n",
    "    try: \n",
    "        inputFileName = join(inputLexFilePath, file)\n",
    "        textToBraille(inputFileName,outputLexPath)\n",
    "    except:\n",
    "        print('file ' + file + ' got an exception')\n",
    "        continue\n",
    "        \n",
    "for file in allTextFiles:\n",
    "    try: \n",
    "        inputFileName = join(inputTextFilePath, file)\n",
    "        textToBraille(inputFileName,outputTextPath)\n",
    "    except:\n",
    "        print('file ' + file + ' got an exception')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
